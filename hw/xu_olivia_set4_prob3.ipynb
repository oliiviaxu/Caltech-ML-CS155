{"cells":[{"cell_type":"markdown","metadata":{"id":"BJ9QqZunbcg6"},"source":["# Problem 3"]},{"cell_type":"markdown","metadata":{"id":"Ohf80Yakbcg9"},"source":["Use this notebook to write your code for problem 3."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XVFUoPubcg9"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"IpunjP-ibcg-"},"source":["## 3D - Convolutional network"]},{"cell_type":"markdown","metadata":{"id":"0LJxk7Xsbcg-"},"source":["As in problem 2, we have conveniently provided for your use code that loads and preprocesses the MNIST data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7IYqIXK6bcg_","executionInfo":{"status":"ok","timestamp":1706900881567,"user_tz":480,"elapsed":1685,"user":{"displayName":"Olivia Xu","userId":"14240237011582997876"}},"outputId":"d016bbdd-58df-46a8-d112-3fd848ca5ea8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 121816451.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 33178771.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 156220722.76it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 13646510.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n"]}],"source":["# load MNIST data into PyTorch format\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","# set batch size\n","batch_size = 32\n","\n","# load training data downloaded into data/ folder\n","mnist_training_data = torchvision.datasets.MNIST('data/', train=True, download=True,\n","                                                transform=transforms.ToTensor())\n","# transforms.ToTensor() converts batch of images to 4-D tensor and normalizes 0-255 to 0-1.0\n","training_data_loader = torch.utils.data.DataLoader(mnist_training_data,\n","                                                  batch_size=batch_size,\n","                                                  shuffle=True)\n","\n","# load test data\n","mnist_test_data = torchvision.datasets.MNIST('data/', train=False, download=True,\n","                                                transform=transforms.ToTensor())\n","test_data_loader = torch.utils.data.DataLoader(mnist_test_data,\n","                                                  batch_size=batch_size,\n","                                                  shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"URtfeM25bcg_","executionInfo":{"status":"ok","timestamp":1706900883030,"user_tz":480,"elapsed":175,"user":{"displayName":"Olivia Xu","userId":"14240237011582997876"}},"outputId":"1aa89295-a438-4337-8462-f6f1f61943dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["1875 training batches\n","60000 training samples\n","313 validation batches\n"]}],"source":["# look at the number of batches per epoch for training and validation\n","print(f'{len(training_data_loader)} training batches')\n","print(f'{len(training_data_loader) * batch_size} training samples')\n","print(f'{len(test_data_loader)} validation batches')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-y9b9bZEbchA"},"outputs":[],"source":["# sample model\n","import torch.nn as nn\n","\n","model = nn.Sequential(\n","    nn.Conv2d(1, 8, kernel_size=(3,3)),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.5),\n","\n","    nn.Conv2d(8, 8, kernel_size=(3,3)),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.5),\n","\n","    nn.Flatten(),\n","    nn.Linear(25*8, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 10)\n","    # PyTorch implementation of cross-entropy loss includes softmax layer\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-HeNb5FbchB","outputId":"7614f8f4-1a4f-4050-f573-640ef9a14d12","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706900886110,"user_tz":480,"elapsed":178,"user":{"displayName":"Olivia Xu","userId":"14240237011582997876"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 1, 3, 3])\n","torch.Size([8])\n","torch.Size([8, 8, 3, 3])\n","torch.Size([8])\n","torch.Size([64, 200])\n","torch.Size([64])\n","torch.Size([10, 64])\n","torch.Size([10])\n"]}],"source":["# why don't we take a look at the shape of the weights for each layer\n","for p in model.parameters():\n","    print(p.data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"daN5DMwFbchB","outputId":"894a9e03-998f-4b40-c589-de14c349300e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706900886857,"user_tz":480,"elapsed":156,"user":{"displayName":"Olivia Xu","userId":"14240237011582997876"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["total params: 14178\n"]}],"source":["# our model has some # of parameters:\n","count = 0\n","for p in model.parameters():\n","    n_params = np.prod(list(p.data.shape)).item()\n","    count += n_params\n","print(f'total params: {count}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FvvBRDubchB"},"outputs":[],"source":["# For a multi-class classification problem\n","import torch.optim as optim\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.RMSprop(model.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P96LnjLLbchC","outputId":"27bc74d6-5f5e-46f7-c5cc-de4b307f8d61","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706901105538,"user_tz":480,"elapsed":216297,"user":{"displayName":"Olivia Xu","userId":"14240237011582997876"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10:...........\n","\tloss: 0.6516, acc: 0.7884, val loss: 0.1586, val acc: 0.9552\n","Epoch 2/10:...........\n","\tloss: 0.4088, acc: 0.8720, val loss: 0.1800, val acc: 0.9496\n","Epoch 3/10:...........\n","\tloss: 0.3858, acc: 0.8797, val loss: 0.1453, val acc: 0.9558\n","Epoch 4/10:...........\n","\tloss: 0.3809, acc: 0.8823, val loss: 0.1391, val acc: 0.9595\n","Epoch 5/10:...........\n","\tloss: 0.3743, acc: 0.8857, val loss: 0.1364, val acc: 0.9595\n","Epoch 6/10:...........\n","\tloss: 0.3605, acc: 0.8892, val loss: 0.1474, val acc: 0.9601\n","Epoch 7/10:...........\n","\tloss: 0.3478, acc: 0.8943, val loss: 0.2071, val acc: 0.9449\n","Epoch 8/10:...........\n","\tloss: 0.3642, acc: 0.8898, val loss: 0.2053, val acc: 0.9456\n","Epoch 9/10:...........\n","\tloss: 0.3616, acc: 0.8902, val loss: 0.1517, val acc: 0.9584\n","Epoch 10/10:...........\n","\tloss: 0.3650, acc: 0.8888, val loss: 0.1834, val acc: 0.9518\n"]}],"source":["# Train the model for 10 epochs, iterating on the data in batches\n","n_epochs = 10\n","\n","# store metrics\n","training_accuracy_history = np.zeros([n_epochs, 1])\n","training_loss_history = np.zeros([n_epochs, 1])\n","validation_accuracy_history = np.zeros([n_epochs, 1])\n","validation_loss_history = np.zeros([n_epochs, 1])\n","\n","for epoch in range(n_epochs):\n","    print(f'Epoch {epoch+1}/10:', end='')\n","    train_total = 0\n","    train_correct = 0\n","    # train\n","    model.train()\n","    for i, data in enumerate(training_data_loader):\n","        images, labels = data\n","        optimizer.zero_grad()\n","        # forward pass\n","        output = model(images)\n","        # calculate categorical cross entropy loss\n","        loss = criterion(output, labels)\n","        # backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # track training accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","        # track training loss\n","        training_loss_history[epoch] += loss.item()\n","        # progress update after 180 batches (~1/10 epoch for batch size 32)\n","        if i % 180 == 0: print('.',end='')\n","    training_loss_history[epoch] /= len(training_data_loader)\n","    training_accuracy_history[epoch] = train_correct / train_total\n","    print(f'\\n\\tloss: {training_loss_history[epoch,0]:0.4f}, acc: {training_accuracy_history[epoch,0]:0.4f}',end='')\n","\n","    # validate\n","    test_total = 0\n","    test_correct = 0\n","    with torch.no_grad():\n","        model.eval()\n","        for i, data in enumerate(test_data_loader):\n","            images, labels = data\n","            # forward pass\n","            output = model(images)\n","            # find accuracy\n","            _, predicted = torch.max(output.data, 1)\n","            test_total += labels.size(0)\n","            test_correct += (predicted == labels).sum().item()\n","            # find loss\n","            loss = criterion(output, labels)\n","            validation_loss_history[epoch] += loss.item()\n","        validation_loss_history[epoch] /= len(test_data_loader)\n","        validation_accuracy_history[epoch] = test_correct / test_total\n","    print(f', val loss: {validation_loss_history[epoch,0]:0.4f}, val acc: {validation_accuracy_history[epoch,0]:0.4f}')"]},{"cell_type":"markdown","metadata":{"id":"HkT0Sf7xbchC"},"source":["Above, we output the training loss/accuracy as well as the validation loss and accuracy. Not bad! Let's see if you can do better."]},{"cell_type":"markdown","source":["## **My Model**"],"metadata":{"id":"rYzkGJm1cjHQ"}},{"cell_type":"code","source":["import torch.nn as nn\n","import numpy as np\n","import torch.optim as optim"],"metadata":{"id":"CIfsQ0s0eRlU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs = np.linspace(0.1,1,10)"],"metadata":{"id":"Lq0bX4H8ch5a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Trying different dropout rates\n","for pr in probs:\n","  print('dropout: {}'.format(pr))\n","  model = nn.Sequential(\n","        nn.Conv2d(1, 16, kernel_size=(3,3)),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2),\n","        nn.Dropout(p=pr),\n","\n","        nn.Conv2d(16, 8, kernel_size=(3,3)),\n","        nn.BatchNorm2d(8),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2),\n","        nn.Dropout(p=pr),\n","\n","        nn.Flatten(),\n","        nn.Linear(25*8, 64),\n","        nn.ReLU(),\n","        nn.Linear(64, 10)\n","    )\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.RMSprop(model.parameters())\n","\n","  # store metrics\n","  training_accuracy_history = np.zeros([len(probs), 1])\n","  training_loss_history = np.zeros([len(probs), 1])\n","  validation_accuracy_history = np.zeros([len(probs), 1])\n","  validation_loss_history = np.zeros([len(probs), 1])\n","\n","  train_total = 0\n","  train_correct = 0\n","  # train\n","  model.train()\n","  for i, data in enumerate(training_data_loader):\n","      images, labels = data\n","      optimizer.zero_grad()\n","      # forward pass\n","      output = model(images)\n","      # calculate categorical cross entropy loss\n","      loss = criterion(output, labels)\n","      # backward pass\n","      loss.backward()\n","      optimizer.step()\n","\n","      # track training accuracy\n","      _, predicted = torch.max(output.data, 1)\n","      train_total += labels.size(0)\n","      train_correct += (predicted == labels).sum().item()\n","      # track training loss\n","      training_loss_history[epoch] += loss.item()\n","      # progress update after 180 batches (~1/10 epoch for batch size 32)\n","      if i % 180 == 0: print('.',end='')\n","  training_loss_history[epoch] /= len(training_data_loader)\n","  training_accuracy_history[epoch] = train_correct / train_total\n","  print(f'\\n\\tloss: {training_loss_history[epoch,0]:0.4f}, acc: {training_accuracy_history[epoch,0]:0.4f}',end='')\n","\n","  # validate\n","  test_total = 0\n","  test_correct = 0\n","  with torch.no_grad():\n","      model.eval()\n","      for i, data in enumerate(test_data_loader):\n","          images, labels = data\n","          # forward pass\n","          output = model(images)\n","          # find accuracy\n","          _, predicted = torch.max(output.data, 1)\n","          test_total += labels.size(0)\n","          test_correct += (predicted == labels).sum().item()\n","          # find loss\n","          loss = criterion(output, labels)\n","          validation_loss_history[epoch] += loss.item()\n","      validation_loss_history[epoch] /= len(test_data_loader)\n","      validation_accuracy_history[epoch] = test_correct / test_total\n","  print(f', val loss: {validation_loss_history[epoch,0]:0.4f}, val acc: {validation_accuracy_history[epoch,0]:0.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRLYP44Gg36S","executionInfo":{"status":"ok","timestamp":1706902607151,"user_tz":480,"elapsed":289179,"user":{"displayName":"Olivia Xu","userId":"14240237011582997876"}},"outputId":"2d90eb76-c82d-4561-b6b1-57b93573f569"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dropout: 0.1\n","...........\n","\tloss: 0.2462, acc: 0.9266, val loss: 0.0728, val acc: 0.9775\n","dropout: 0.2\n","...........\n","\tloss: 0.2673, acc: 0.9237, val loss: 0.0979, val acc: 0.9691\n","dropout: 0.30000000000000004\n","...........\n","\tloss: 0.3554, acc: 0.8909, val loss: 0.1040, val acc: 0.9676\n","dropout: 0.4\n","...........\n","\tloss: 0.4143, acc: 0.8719, val loss: 0.1171, val acc: 0.9635\n","dropout: 0.5\n","...........\n","\tloss: 0.4921, acc: 0.8458, val loss: 0.1527, val acc: 0.9563\n","dropout: 0.6\n","...........\n","\tloss: 0.6477, acc: 0.7896, val loss: 0.1758, val acc: 0.9472\n","dropout: 0.7000000000000001\n","...........\n","\tloss: 0.8653, acc: 0.7180, val loss: 0.3300, val acc: 0.9195\n","dropout: 0.8\n","...........\n","\tloss: 1.2097, acc: 0.5922, val loss: 0.4944, val acc: 0.8699\n","dropout: 0.9\n","...........\n","\tloss: 2.0022, acc: 0.2823, val loss: 2.0161, val acc: 0.2158\n","dropout: 1.0\n","...........\n","\tloss: 2.3028, acc: 0.1074, val loss: 84.4860, val acc: 0.0982\n"]}]},{"cell_type":"code","source":["#final model\n","model = nn.Sequential(\n","      nn.Conv2d(1, 16, kernel_size=(3,3)),\n","      nn.ReLU(),\n","      nn.MaxPool2d(2),\n","      nn.Dropout(p=0.1),\n","\n","      nn.Conv2d(16, 8, kernel_size=(3,3)),\n","      nn.BatchNorm2d(8),\n","      nn.ReLU(),\n","      nn.MaxPool2d(2),\n","      nn.Dropout(p=0.1),\n","\n","      nn.Flatten(),\n","      nn.Linear(25*8, 64),\n","      nn.ReLU(),\n","      nn.Linear(64, 10)\n",")\n","\n","# for p in model.parameters():\n","print(p.data.shape)\n","# our model has some # of parameters:\n","count = 0\n","for p in model.parameters():\n","    n_params = np.prod(list(p.data.shape)).item()\n","    count += n_params\n","print(f'total params: {count}')\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.RMSprop(model.parameters())\n","\n","# Train the model for 10 epochs, iterating on the data in batches\n","n_epochs = 10\n","\n","# store metrics\n","training_accuracy_history = np.zeros([n_epochs, 1])\n","training_loss_history = np.zeros([n_epochs, 1])\n","validation_accuracy_history = np.zeros([n_epochs, 1])\n","validation_loss_history = np.zeros([n_epochs, 1])\n","\n","for epoch in range(n_epochs):\n","    print(f'Epoch {epoch+1}/10:', end='')\n","    train_total = 0\n","    train_correct = 0\n","    # train\n","    model.train()\n","    for i, data in enumerate(training_data_loader):\n","        images, labels = data\n","        optimizer.zero_grad()\n","        # forward pass\n","        output = model(images)\n","        # calculate categorical cross entropy loss\n","        loss = criterion(output, labels)\n","        # backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # track training accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","        # track training loss\n","        training_loss_history[epoch] += loss.item()\n","        # progress update after 180 batches (~1/10 epoch for batch size 32)\n","        if i % 180 == 0: print('.',end='')\n","    training_loss_history[epoch] /= len(training_data_loader)\n","    training_accuracy_history[epoch] = train_correct / train_total\n","    print(f'\\n\\tloss: {training_loss_history[epoch,0]:0.4f}, acc: {training_accuracy_history[epoch,0]:0.4f}',end='')\n","\n","    # validate\n","    test_total = 0\n","    test_correct = 0\n","    with torch.no_grad():\n","        model.eval()\n","        for i, data in enumerate(test_data_loader):\n","            images, labels = data\n","            # forward pass\n","            output = model(images)\n","            # find accuracy\n","            _, predicted = torch.max(output.data, 1)\n","            test_total += labels.size(0)\n","            test_correct += (predicted == labels).sum().item()\n","            # find loss\n","            loss = criterion(output, labels)\n","            validation_loss_history[epoch] += loss.item()\n","        validation_loss_history[epoch] /= len(test_data_loader)\n","        validation_accuracy_history[epoch] = test_correct / test_total\n","    print(f', val loss: {validation_loss_history[epoch,0]:0.4f}, val acc: {validation_accuracy_history[epoch,0]:0.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CkRcuGms9G8u","executionInfo":{"status":"ok","timestamp":1706902010766,"user_tz":480,"elapsed":296568,"user":{"displayName":"Olivia Xu","userId":"14240237011582997876"}},"outputId":"51fe0c5c-1aed-48e1-e768-6400aa350417"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10])\n","total params: 14850\n","Epoch 1/10:...........\n","\tloss: 0.2303, acc: 0.9309, val loss: 0.0824, val acc: 0.9729\n","Epoch 2/10:...........\n","\tloss: 0.1120, acc: 0.9650, val loss: 0.0858, val acc: 0.9716\n","Epoch 3/10:...........\n","\tloss: 0.0983, acc: 0.9694, val loss: 0.0614, val acc: 0.9811\n","Epoch 4/10:...........\n","\tloss: 0.0902, acc: 0.9720, val loss: 0.0765, val acc: 0.9763\n","Epoch 5/10:...........\n","\tloss: 0.0854, acc: 0.9740, val loss: 0.0718, val acc: 0.9782\n","Epoch 6/10:...........\n","\tloss: 0.0836, acc: 0.9743, val loss: 0.0506, val acc: 0.9851\n","Epoch 7/10:...........\n","\tloss: 0.0813, acc: 0.9746, val loss: 0.0623, val acc: 0.9812\n","Epoch 8/10:...........\n","\tloss: 0.0760, acc: 0.9767, val loss: 0.0498, val acc: 0.9845\n","Epoch 9/10:...........\n","\tloss: 0.0756, acc: 0.9767, val loss: 0.0621, val acc: 0.9811\n","Epoch 10/10:...........\n","\tloss: 0.0731, acc: 0.9786, val loss: 0.0469, val acc: 0.9869\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}